# -*- coding: utf-8 -*-
"""Zero bits reduction Pytorch cifar10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nbc-vtW2uyc4Rl5rcqY8V31e6yFTqWBk
"""

#@title installing Pytorch

#!pip install torch
#!pip install torchvision

#@title Import Dependencies

import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable

#@title Define Hyperparameters


num_classes = 10 # number of output classes discrete range [0,9]
num_epochs = 4 # number of times which the entire dataset is passed throughout the model
batch_size = 100 # the size of input data took for one iteration
lr = 1e-3 # size of step

#@title Downloading MNIST data
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./cifardata', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./cifardata', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=500,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

#@title Build the model
torch.manual_seed(1234)
import numpy as np
np.random.seed(1234)

#@title Define model class
INVBER=10000000
import struct
from ctypes import *

import torch.nn.functional as F

def ber_uniform_np (x_input,seeds_arr): #x_input is a np array XOR 1 = flip.
  #random.shuffle(seeds_arr)

  new_input = x_input.flatten()
  i = 0
  seed_arr_idx = 0
  while i < len(new_input):
    if (seed_arr_idx == len(seeds_arr)):
      random.shuffle(seeds_arr)
      seed_arr_idx = 0
    bits = cast(pointer(c_float(new_input[i])), POINTER(c_int32)).contents.value
    bits = bits ^ seeds_arr[seed_arr_idx]
    new_input[i] = cast(pointer(c_int32(bits)), POINTER(c_float)).contents.value
    i = i +1
    seed_arr_idx = seed_arr_idx+1

  return new_input.reshape(x_input.shape)

import random
random.seed()
total_elements = 10000000
total_bits = total_elements * 32
ones = int(float(total_bits)/INVBER)
print ("total bits ", total_bits)
print ("ones bit ", ones)
'''
zeroes = total_bits - ones
ones_list = ['1']*ones
zeroes_lists = ['0']*zeroes
ones_list.extend(zeroes_lists)
full_list = np.array(ones_list)
assert(len (full_list) == total_bits)
random.shuffle(full_list)
full_list = full_list.reshape(-1,32)
#print (full_list)
int_list = np.apply_along_axis(lambda x: int("".join(x),2), 1 , full_list)
'''

int_list = []
for i in range(ones):
    indx = random.randint(0,31)
    if (indx == 31): # signed int problem of pytorch tensor, no such type
        int_list.append(-2147483648)
    else:
        int_list.append(1<<indx)
print (" int list ",int_list)
int_tensor = torch.zeros(total_elements, dtype=torch.int32)
for i in range(ones):
    int_tensor[random.randint(0,total_elements-1)] = int_list[i]
int_tensor = int_tensor.cuda()

from torch.utils.cpp_extension import load
custom_module = load(name='ber_uniform', sources=['flip_bit_cuda.cpp', 'flip_bit_cuda_kernel.cu'])

#def ber_uniform_tensor (x_input): #input tensor - > tensor (GPU)
#  return (x_input.cpu().detach().apply_(ber_uniform)).cuda()

def ber_uniform_tensor_fast (x_input): #input tensor - > tensor (GPU)
#x_input = custom_module.ber_uniform(x_input,int_tensor)
  new_input = x_input.cpu().detach().numpy()
  return torch.from_numpy(ber_uniform_np(new_input, int_list)).cuda()





class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

from scipy import stats
class NetBER(nn.Module):
    def __init__(self):
        super(NetBER, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        #x = ber_uniform_tensor_fast(x)
        #x1 = x.clone()
        #x2 = x.clone()
        x = custom_module.ber_uniform(x,int_tensor)
        #x2 = ber_uniform_tensor_fast(x2)
        #print (x1-x2)
        #exit()
        x = self.pool(F.relu(self.conv1(x)))
        x = custom_module.ber_uniform(x,int_tensor)
        #x = ber_uniform_tensor_fast(x)
        x = self.pool(F.relu(self.conv2(x)))
        x = custom_module.ber_uniform(x,int_tensor)
        #x = ber_uniform_tensor_fast(x)
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = custom_module.ber_uniform(x,int_tensor)
        #x = ber_uniform_tensor_fast(x)
        x = F.relu(self.fc2(x))
        x = custom_module.ber_uniform(x,int_tensor)
        #x = ber_uniform_tensor_fast(x)
        x = self.fc3(x)
        return x

net = Net()

import struct
#getting binary representatation of num
def binary(num):
    #python2
    #return ''.join(bin(ord(c)).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', num))
    return ''.join('{:0>8b}'.format(c) for c in struct.pack('!f', num))
def count_zerobit (data_arr):
  total_bit = len(data_arr)*32
  #print ("total_bit ", total_bit)
  zero_bits = 0
  for item in data_arr:
    binary_form = binary(item)
    zero_bits += binary_form.count('0')
  return zero_bits/float(total_bit)

#test
print (count_zerobit([1]))
print (count_zerobit([1.2,1.7]))

print ("done ")

net = Net()
if torch.cuda.is_available():
  net.cuda()
print ("done ")

#@title Define loss-function & optimizer

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam( net.parameters(), lr=1e-3)
#optimizer = MyAdam( net.parameters(), lr=1e-3)
print ("done, lr = ", lr )

# Commented out IPython magic to ensure Python compatibility.
#@title Training the model
'''
num_epochs = 10
for epoch in range(num_epochs):
  for i ,(images,labels) in enumerate(trainloader):
    images = Variable(images).cuda()
    labels = Variable(labels).cuda()

    optimizer.zero_grad()
    outputs = net(images)
    loss = loss_function(outputs, labels)
    #all_params = []
    #for param in net.parameters():
    #  all_params.extend(param.data.cpu().numpy().flatten())
    #zero_bit = count_zerobit(all_params)
    #loss = loss + zero_bit
    loss.backward()
    optimizer.step()

    if (i+1) % 60 == 0:
      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'
                  %(epoch+1, num_epochs, i+1, len(trainloader)//batch_size, loss.data))

print ("done training")
state_dict = net.state_dict()

for name, param in state_dict.items():
  print(name)

torch.save(net.state_dict(), "cifar.model")

#@title Evaluating the accuracy of the model
net = Net()
net.load_state_dict(torch.load("cifar.model"))
if torch.cuda.is_available():
  net.cuda()
print ("loaded trained weights ")
print (len(testloader))

correct = 0
total = 0
count = 0
for images,labels in testloader:
  count = count +1
  if (count == 10) :
    break
  print ('1 batch')
  images = Variable(images).cuda()
  labels = labels.cuda()

  output = net(images)
  _, predicted = torch.max(output,1)
  correct += (predicted == labels).sum()
  total += labels.size(0)

print('Original accuracy of the model: %.3f %%' %((100*correct)/(total+1)))
#params = list(net.parameters())
#print (params)
'''


net = NetBER()
net.load_state_dict(torch.load("cifar.model"))
if torch.cuda.is_available():
  net.cuda()
print ("loaded trained weights ")
print (len(testloader))

correct = 0
total = 0
count = 0
for images,labels in testloader:
  count = count +1
  if (count == 10) :
    break
  print ('1 batch')
  images = Variable(images).cuda()
  labels = labels.cuda()

  output = net(images)
  _, predicted = torch.max(output,1)
  correct += (predicted == labels).sum()
  total += labels.size(0)

print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))
#params = list(net.parameters())
#print (params)

all_params = []
for param in net.parameters():
  all_params.extend(param.data.cpu().numpy().flatten())

print ("percentage of zero bits ",count_zerobit(all_params))

print ("zero out mantissa ")


correct = 0
total = 0

from ctypes import *

'''
#params = list(net.parameters())
#print (params)
state_dict = net.state_dict()

for name, param in state_dict.items():
  numpy_arr = param.data.cpu().detach().numpy()
  old_shape = numpy_arr.shape
  numpy_arr_new = numpy_arr.flatten()
  for i in range(len(numpy_arr_new)):
    bits = cast(pointer(c_float(numpy_arr_new[i])), POINTER(c_int32)).contents.value
    bits = bits & 0xffc00000
    numpy_arr_new[i] = cast(pointer(c_int32(bits)), POINTER(c_float)).contents.value
  transformed_param = torch.tensor(numpy_arr_new.reshape(old_shape))
    # Update the parameter.
  state_dict[name].copy_(transformed_param)


for images,labels in testloader:
  images = Variable(images).cuda()
  labels = labels.cuda()
  output = net(images)
  _, predicted = torch.max(output,1)
  correct += (predicted == labels).sum()
  total += labels.size(0)

print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))
all_params = []
for param in net.parameters():
  all_params.extend(param.data.cpu().numpy().flatten())

print ("percentage of zero bits ",count_zerobit(all_params))
'''
